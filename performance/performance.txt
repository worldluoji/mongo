0. 数据准备：
mongoimport --drop -c people --uri mongodb+srv://m001-student:xxx@luoji-mongo-cluster.ejuuu.mongodb.net/m201 people.json
mongoimport --drop -c restaurants --uri mongodb+srv://m001-student:xxx@luoji-mongo-cluster.ejuuu.mongodb.net/m201 restaurants.json

1. index
Indexes are used to increase the speed of our queries.
The _id field is automatically indexed on all collections.
Indexes reduce the number of documents MongoDB needs to examine to satisfy a query.
Indexes can decrease write, update, and delete performance.

example: db.trips.createIndex({ "birth year": 1 })
可以给subdoc创建索引，也可以创建联合索引。

db.members.createIndex( { "user_id": 1 }, { unique: true } ) 
则是唯一索引，表示user_id不能重复

2. explain
// switch to the m201 database
use m201

// create an explainable object with no parameters
exp = db.people.explain()
queryPlanner 默认的模式，仅进行查询计划分析， 不会去真正执行查询，无法输出执行过程统计；
executionStats 执行模式，在查询计划分析后，将执行winningPlan并统计过程信息；
allPlansExecution 全计划执行模式，将执行所有计划(包括rejectPlans)，并返回过程统计信息；
executionStats.allPlansExecution 包含了所有计划(除winningPlan之外)的执行过程统计信息

// create an explainable object with the 'executionStats' parameter
expRun = db.people.explain("executionStats")

// and one final explainable object with the 'allPlansExecution' parameter
expRunVerbose = db.people.explain("allPlansExecution")

// execute and explain the query, collecting execution statistics
expRun.find({"last_name":"Johnson", "address.state":"New York"})

// create an index on last_name
db.people.createIndex({last_name:1})

// rerun the query (uses the index)
expRun.find({"last_name":"Johnson", "address.state":"New York"})

// create a compound index
db.people.createIndex({"address.state": 1, last_name: 1})

// rerun the query (uses the new index)
expRun.find({"last_name":"Johnson", "address.state":"New York"})

// run a sort query
var res = db.people.find({"last_name":"Johnson", "address.state":"New York"}).sort({"birthday":1}).explain("executionStats")

// checkout the execution stages (doing an in-memory sort)
res.executionStats.executionStages

3. example
If you observe the following lines in the output of explain('executionStats'), what can you deduce?
"executionStats" : {
  "executionSuccess" : true,
  "nReturned" : 23217,
  "executionTimeMillis" : 91,
  "totalKeysExamined" : 23217,
  "totalDocsExamined" : 23217,
  "executionStages" : {
    "stage" : "SORT",
    "nReturned" : 23217,
    "executionTimeMillisEstimate" : 26,
    "works" : 46437,
    "advanced" : 23217,
    "needTime" : 23219,
    "needYield" : 0,
    "saveState" : 363,
    "restoreState" : 363,
    "isEOF" : 1,
    "sortPattern" : {
      "stars" : 1
    },
    "memUsage" : 32522511,
    "memLimit" : 33554432,

The index selected for the query was not useful for the sort part of the query

Because we have "totalKeysExamined" : 23217, we know an index was used.

The "stage" : "SORT" tells us the index was not use for the sort and the system needed to 
create an additional stage to perform it.

An in-memory sort was performed

The "stage" : "SORT" tells us the index was not use for the sort and a sort had to done, 
so it had to be done in memory.

The system came very close to throwing an exception instead of returning results

Each stage passes its results (i.e. documents or index keys) to the parent node. 
The leaf nodes access the collection or the indices. 
The internal nodes manipulate the documents or the index keys that result from the child nodes. 
The root node is the final stage from which MongoDB derives the result set.

Stages are descriptive of the operation; e.g.

COLLSCAN for a collection scan
IXSCAN for scanning index keys
FETCH for retrieving documents
SHARD_MERGE for merging results from shards
SHARDING_FILTER for filtering out orphan documents from shards

"memUsage" : 32522511, "memLimit" : 33554432 tells us that 32,522,511 bytes out of a maximum available 
of 33,554,432 were used to perform the sort in memory. If the sort operation would have just a few 
more documents, it may have gone over the limit. In this case, the sort stage is aborted and 
an exception is thrown to the client application. This means that you need to avoid operations 
that perform in-memory sort. They may seem to work in your test environment with smaller dataset, 
however, they will fail with a large production dataset.


explain()返回信息:

queryPlanner（查询计划）：查询优化选择的计划细节和被拒绝的计划。其可能包括以下值：
  queryPlanner.namespace－一个字符串，运行查询的指定命名空间
  queryPlanner.indexFilterSet－一个布尔什，表示MongoDB在查询中是否使用索引过滤
  queryPlanner.winningPlan－由查询优化选择的计划文档
  winningPlan.stage－表示查询阶段的字符串
  winningPlan.inputStage－表示子过程的文档
  winningPlan.inputStages－表示子过程的文档数组
  queryPlanner.rejectedPlans－被查询优化备选并被拒绝的计划数组
executionStats,（执行状态）：被选中执行计划和被拒绝执行计划的详细说明：
  queryPlanner.nReturned－匹配查询条件的文档数
  queryPlanner.executionTimeMillis－计划选择和查询执行所需的总时间（毫秒数）
  queryPlanner.totalKeysExamined－扫描的索引总数
  queryPlanner.totalDocsExamined－扫描的文档总数
  queryPlanner.totalDocsExamined－扫描的文档总数
  queryPlanner.executionStages－显示执行成功细节的查询阶段树
  executionStages.works－指定查询执行阶段执行的“工作单元”的数量
  executionStages.advanced－返回的中间结果数
  executionStages.needTime－未将中间结果推进到其父级的工作周期数
  executionStages.needYield－存储层要求查询系统产生的锁的次数
  executionStages.isEOF－指定执行阶段是否已到达流结束
  queryPlanner.allPlansExecution－包含在计划选择阶段期间捕获的部分执行信息，包括选择计划和拒绝计划
serverInfo,（服务器信息）：MongoDB实例的相关信息：
  serverInfo.winningPlan－使用的执行计划
  winningPlan.shards－包括每个访问片的queryPlanner和serverInfo的文档数组


4. shard(分片)
// switch to the m201 database
use m201

// enable sharding on the m201 database
sh.enableSharding("m201")

// shard the people collection on the _id index
sh.shardCollection("m201.people", {_id: 1})

// after the import, check the shard distribution (data should be on both shards)
db.people.getShardDistribution()

// checkout the explain output for a sharded collection
db.people.find({"last_name":"Johnson", "address.state":"New York"}).explain("executionStats")

5. example2
With the output of an explain command, what can you deduce?

1) The index used by the chosen plan

Yes, additional information will be the direction the index is used, the bounds of the values 
looked at and the number of keys examined.

2) If a sort was performed by walking the index or done in memory

Yes.

3) All the available indexes for this collection

No, you will be able to see the ones considered by the other plans that were rejected with the 
"allExecutionPlans" option, but this is possibly only a subset of all indexes.

4) All the different stages the query needs to go through with details about the time it takes, 
the number of documents processed and returned to the next stage in the pipeline

Yes.

5) The estimation of the cardinalities of the distribution of the values

No, while some RDBMS use this kind of statistics to select indexes, MongoDB executes all select plans for a short duration of time and picks the best based on execution results.


6. methods for sorting
1) In memory
2) Using index

// switch to the m201 database
use m201

db.people.createIndex({ ssn: 1 })

// find all documents and sort them by ssn
db.people.find({}, { _id : 0, last_name: 1, first_name: 1, ssn: 1 }).sort({ ssn: 1 })

// create an explainable object for the people collection
var exp = db.people.explain('executionStats')

// and rerun the query (uses the index for sorting)
exp.find({}, { _id : 0, last_name: 1, first_name: 1, ssn: 1 }).sort({ ssn: 1 })

// this time, sort by first_name (didn't use the index for sorting)
exp.find({}, { _id : 0, last_name: 1, first_name: 1, ssn: 1 }).sort({ first_name: 1 })

// and rerun the first query, but sort descending (walks the index backward)
exp.find({}, { _id : 0, last_name: 1, first_name: 1, ssn: 1 }).sort({ ssn: -1 })

// filtering and sorting in the same query (both using the index, backward)
exp.find( { ssn : /^555/ }, { _id : 0, last_name: 1, first_name: 1, ssn: 1 } ).sort( { ssn : -1 } )

// drop all indexes
db.people.dropIndexes()

// create a new descending (instead of ascending) index on ssn
db.people.createIndex({ ssn: -1 })

// rerun the same query, now walking the index forward
exp.find( { ssn : /^555/ }, { _id : 0, last_name: 1, first_name: 1, ssn: 1 } ).sort( { ssn : -1 } )

7. coupon index
1) index prefix
db.xxx.createIndex({ "item": 1, "location": 1, "stock": 1 })

The index has the following index prefixes:

{ item: 1 }
{ item: 1, location: 1 }
For a compound index, MongoDB can use the index to support queries on the index prefixes. 
As such, MongoDB can use the index for queries on the following fields:

the item field,
the item field and the location field,
the item field and the location field and the stock field.

MongoDB和MySQL一样，索引需要遵循左前缀原则。


2) coupon index with sorting:
// confirm you still have an index on job, employer, last_name, & first_name
db.people.dropIndexes
db.people.createIndex({ "job": 1, "employer": 1, "last_name": 1, "first_name": 1})
db.people.getIndexes()

// create an explainable object for the people collection
var exp = db.people.explain("executionStats")

// sort all documents using the verbatim index key pattern
exp.find({}).sort({ job: 1, employer: 1, last_name : 1, first_name : 1 })

// sort all documents using the first two fields of the index (uses the index), 满足左前缀原则
exp.find({}).sort({ job: 1, employer: 1 })

// sort all documents, swapping employer and job (doesn't use the index)
exp.find({}).sort({ employer: 1, job: 1 })

// will still use the index (for sorting)
exp.find({ email:"jenniferfreeman@hotmail.com" }).sort({ job: 1 })

// use the index for filtering and sorting
exp.find({ job: 'Graphic designer', employer: 'Wilson Ltd' }).sort({ last_name: 1 })

// doesn't follow an index prefix, and can't use the index for sorting, only filtering
exp.find({ job: 'Graphic designer' }).sort({ last_name: 1 })

// create a new compound index
db.coll.createIndex({ a: 1, b: -1, c: 1 })

// walk the index forward
db.coll.find().sort({ a: 1, b: -1, c: 1 })

// walk the index backward, by inverting the sort predicate
db.coll.find().sort({ a: -1, b: 1, c: -1 })

// all of these queries use the index for sorting
db.coll.find().sort({ a: 1 })
db.coll.find().sort({ a: 1, b: -1 })
db.coll.find().sort({ a: -1 })
db.coll.find().sort({ a: -1, b: 1 })

// uses the index for sorting
exp.find().sort({job: -1, employer: -1})

// sorting is done in-memory
exp.find().sort({job: -1, employer: 1})

8. multikey index
// switch to the m201 database
use m201

// insert a document into the products collection
db.products.insert({
  productName: "MongoDB Short Sleeve T-Shirt",
  categories: ["T-Shirts", "Clothing", "Apparel"],
  stock: { size: "L", color: "green", quantity: 100 }
});

// create an index on stock.quantity
db.products.createIndex({ "stock.quantity": 1})

// create an explainable object on the products collection
var exp = db.products.explain()

// look at the explain output for the query (uses an index, isMultiKey is false)
exp.find({ "stock.quantity": 100 })

// insert a document where stock is now an array
db.products.insert({
  productName: "MongoDB Long Sleeve T-Shirt",
  categories: ["T-Shirts", "Clothing", "Apparel"],
  stock: [
    { size: "S", color: "red", quantity: 25 },
    { size: "S", color: "blue", quantity: 10 },
    { size: "M", color: "blue", quantity: 50 }
  ]
});

有数组才会有multikey index
// rerun our same query (still uses an index, but isMultiKey is now true)
exp.find({ "stock.quantity": 100 })

multikey index不能包含两个数组
// creating an index on two array fields will fail
db.products.createIndex({ categories: 1, "stock.quantity": 1 })

// but compound indexes with only 1 array field are good
db.products.createIndex({ productName: 1, "stock.quantity": 1 })

// productName can be an array if stock isn't
db.products.insert({
  productName: [
    "MongoDB Short Sleeve T-Shirt",
    "MongoDB Short Sleeve Shirt"
  ],
  categories: ["T-Shirts", "Clothing", "Apparel"],
  stock: { size: "L", color: "green", quantity: 100 }
});

// but this will fail, because both productName and stock are arrays
db.products.insert({
  productName: [
    "MongoDB Short Sleeve T-Shirt",
    "MongoDB Short Sleeve Shirt"
  ],
  categories: ["T-Shirts", "Clothing", "Apparel"],
  stock: [
    { size: "S", color: "red", quantity: 25 },
    { size: "S", color: "blue", quantity: 10 },
    { size: "M", color: "blue", quantity: 50 }
  ]
});

9. partial index
// switch to the m201 database
use m201

// insert a restaurant document
db.restaurants.insert({
   "name" : "Han Dynasty",
   "cuisine" : "Sichuan",
   "stars" : 4.4,
   "address" : {
      "street" : "90 3rd Ave",
      "city" : "New York",
      "state" : "NY",
      "zipcode" : "10003"
   }
});

// and run a find query on city and cuisine
db.restaurants.find({'address.city': 'New York', 'cuisine': 'Sichuan'})

// create an explainable object
var exp = db.restaurants.explain()

// and rerun the query
exp.find({'address.city': 'New York', cuisine: 'Sichuan'})

// create a partial index
db.restaurants.createIndex(
  { "address.city": 1, cuisine: 1 },
  { partialFilterExpression: { 'stars': { $gte: 3.5 } } }
)
这样，只有starts >= 3.5的document才会建立索引。

// rerun the query (doesn't use the partial index)
db.restaurants.find({'address.city': 'New York', 'cuisine': 'Sichuan'})

// adding the stars predicate allows us to use the partial index
exp.find({'address.city': 'New York', cuisine: 'Sichuan', stars: { $gt: 4.0 }})

All of the following are true:
Partial indexes represent a superset of the functionality of sparse indexes.
Partial indexes can be used to reduce the number of keys in an index.
Partial indexes support compound indexes.

The following is not true:
Partial indexes don't support a uniqueness constraint.
No, you can still specify a uniqueness constraint with a partial index. 
However, uniqueness will be limited to the keys covered by the partial filter expression.

10. Sparse indexes:

These can be particularly useful for fields that are optional but which should also be unique.
db.emails.insert([
{ "_id" : "john@example.com", "nickname" : "Johnnie" },
{ "_id" : "jane@example.com" },
{ "_id" : "julia@example.com", "nickname" : "Jules"},
{ "_id" : "jack@example.com" }])
Since two entries have no "nickname" specified and indexing will treat unspecified fields as null, 
the index creation would fail with 2 documents having 'null', so:

db.scores.createIndex( { nickname: 1 } , { unique: true, sparse: true } )
will let you still have 'null' nicknames.

Sparse indexes are more compact since they skip/ignore documents that don't specify that field. 
So if you have a collection where only less than 10% of documents specify this field, 
you can create much smaller indexes - making better use of limited memory if you want to do queries like:

db.scores.find({'nickname': 'Johnnie'})

10. text index
text index 会对字符串进行分词处理，就像Elasticsearch一样

// switch to the m201 database
use m201

// insert 2 example documents
db.textExample.insertOne({ "statement": "MongoDB is the best" })
db.textExample.insertOne({ "statement": "MongoDB is the worst." })

// create a text index on "statement"
db.textExample.createIndex({ statement: "text" })

// Search for the phrase "MongoDB best"
db.textExample.find({ $text: { $search: "mongodb best" } })
{ "_id" : ObjectId("61c95c62c95ed8763a96c4eb"), "statement" : "MongoDB is the best" }
{ "_id" : ObjectId("61c95c71c95ed8763a96c4ec"), "statement" : "MongoDB is the worst." }


> db.textExample.find({$text: {$search: "mongodb"}})
{ "_id" : ObjectId("61c95c71c95ed8763a96c4ec"), "statement" : "MongoDB is the worst." }
{ "_id" : ObjectId("61c95c62c95ed8763a96c4eb"), "statement" : "MongoDB is the best" }

// Display each document with it's "textScore"
db.textExample.find({ $text: { $search : "MongoDB best" } }, { score: { $meta: "textScore" } })
{ "_id" : ObjectId("61c95c62c95ed8763a96c4eb"), "statement" : "MongoDB is the best", "score" : 1.5 }
{ "_id" : ObjectId("61c95c71c95ed8763a96c4ec"), "statement" : "MongoDB is the worst.", "score" : 0.75 }


// Sort the documents by their textScore so that the most relevant documents return first
db.textExample.find({ $text: { $search : "MongoDB best" } }, { score: { $meta: "textScore" } }).sort({ score: { $meta: "textScore" } })


11. collation
Collation allows users to specify language-specific rules for string comparison, 
such as rules for lettercase and accent marks.

You can specify collation for a collection or a view, an index, or specific operations
that support collation.

简而言之，Collation特性允许MongoDB的用户根据不同的语言定制排序规则.

// switch to the m201 database
use m201

// create a collection-level collation for Portuguese
db.createCollection( "foreign_text", {collation: {locale: "pt"}})

// insert an example document
db.foreign_text.insert({ "name": "Máximo", "text": "Bom dia minha gente!"})

// explain the following query (uses the Portuguese collation)
db.foreign_text.find({ _id: {$exists:1 } } ).explain()

// specify an Italian collation for a find query
db.foreign_text.find({ _id: {$exists:1 } }).collation({locale: 'it'})

// specify a Spanish collation for an aggregation query
db.foreign_text.aggregate([ {$match: { _id: {$exists:1 }  }}], {collation: {locale: 'es'}})

// create an index with a collation that differs from the collection collation
db.foreign_text.createIndex( {name: 1},  {collation: {locale: 'it'}} )

// uses the collection collation (Portuguese)， can not use index
db.foreign_text.find( {name: 'Máximo'}).explain()

// uses the index collation (Italian)
db.foreign_text.find( {name: 'Máximo'}).collation({locale: 'it'}).explain()


// create a case-insensitive index via collations
db.createCollection( "no_sensitivity", {collation: {locale: 'en', strength: 1}})
"strength: 1" means that Collation performs comparisons of the base characters only, 
ignoring other differences such as diacritics and case.

这里还有要给caseLevel参数：
caseLevel boolean
Optional. Flag that determines whether to include case comparison at strength level 1 or 2.
If true, include case comparison; i.e.
When used with strength:1, collation compares base characters and case.
When used with strength:2, collation compares base characters, diacritics (and possible other secondary differences) and case.
If false, do not include case comparison at level 1 or 2. The default is false.

// insert some documents
db.no_sensitivity.insert({name: 'aaaaa'})
db.no_sensitivity.insert({name: 'aAAaa'})
db.no_sensitivity.insert({name: 'AaAaa'})

// sort them by name ascending
db.no_sensitivity.find().sort({name:1})

// even if we change the sort-order, the documents will be returned in the same
// order because of the case-insensitive collation
db.no_sensitivity.find().sort({name:-1})

https://docs.mongodb.com/manual/reference/collation/?jmp=university

12. Wildcard index
MongoDB 4.2 introduces wildcard indexes for supporting queries against unknown or arbitrary fields.

Wildcard indexes are a new index type in MongoDB, which, as their name suggests,
allow you to dynamically create indexes on all fields or a selected subset of fields 
for each document in a collection.

Here at MongoDB, we always say that you should only index fields that your queries frequently access.
And that still applies.
Wildcard indexes aren't a replacement for traditional indexes.
So remember, indexes are great for query performance, but come at a cost.

why would we index everything?
For instance, sensor data for IoT use cases or weather stations.
In such cases, each query may include a combination of an arbitrarily large number of different fields.
This can make it very difficult to plan an effective indexing strategy.
For these workloads, we needed a way to be able to index on multiple fields without the overhead 
of maintaining multiple indexes.

b.userData.createIndex( { "userMetadata.$**" : 1 } )

The index can support the following queries:
db.userData.find({ "userMetadata.likes" : "dogs" })
db.userData.find({ "userMetadata.dislikes" : "pickles" })
db.userData.find({ "userMetadata.age" : { $gt : 30 } })
db.userData.find({ "userMetadata" : "inactive" })


db.collection.createIndex(
  { "$**" : 1 },
  { "wildcardProjection" :
    { "fieldA" : 1, "fieldB.fieldC" : 1 }
  }
)

With this wildcard index, MongoDB indexes all values for the specified fields for each document
in the collection. If a given field is a nested document or array, the wildcard index recurses
into the document/array and stores the value for all fields in the document/array.
“"fieldA" : 0”则表示exclude.

13. index builds
Previous versions of MongoDB supported building indexes either in the foreground or background. 
Foreground index builds were fast and produced more efficient index data structures, 
but required blocking all read-write access to the parent database of the collection being indexed 
for the duration of the build. Background index builds were slower and had less efficient results, 
but allowed read-write access to the database and its collections during the build process.

Starting in MongoDB 4.2, index builds obtain an exclusive lock on only the collection being indexed 
during the start and end of the build process to protect metadata changes. 
The rest of the build process uses the yielding behavior of background index builds 
to maximize read-write access to the collection during the build. 
it is a hybrid mechanism.
4.2 index builds still produce efficient index data structures despite the more permissive 
locking behavior.

The new hybrid index build has both the performance of a foreground index build and 
the nonlocking properties of a background index build, meaning that 
all database operations can proceed uninhibited for the duration of the build.

This is now the only way to build an index on MongoDB.

14. query plans
For a query, the MongoDB query optimizer chooses and caches the most efficient query plan 
given the available indexes. 
The evaluation of the most efficient query plan is based on the number of "work units" (works)
performed by the query execution plan when the query planner evaluates candidate plans.

The associated plan cache entry is used for subsequent queries with the same query shape.

When a fresh query comes into the database for the first time, 
the server is going to look at all the available indexes on the collection.
From there, it will identify which indexes are viable to satisfy the query.

We call these candidate indexes.
From these candidate indexes, the query optimizer can generate candidate plans.

Query plans are removed from the plan cache on index creation, destruction, or server restart.

Now MongoDB has what is called an empirical query planner, which means that there is going to be 
a trial period, where each of the candidate plans is executed over a short period of time.
And the planner will then see which plan performed best.

I don't want to get too into the weeds here, but best can be which plan returned all the results first.
Or it might be, which returned a certain number of documents in store order fastest.

If we were to run explain and look under the winning plan field, this is the plan it would be talking about.

15. Forcing Indexes with Hint
db.users.find({"username":"user1000", "age":30}).hint({"username":1})
hint()强制使用"username":1为索引
需要说明的是，mongodb的优化器已经很牛逼了，非必要无需使用hint()

16. 资源占用情况查看
1）查看collection的情况：
db.people.stats(); 
Atlas atlas-zdff09-shard-0 [primary] m201> db.people.stats();
db.stats()
{
  "db" : "people",
  "collections" : 5, 表示当前数据库有多少个collections.可以通过运行show collections查看
  "objects" : 22,    表示当前数据库所有collection总共有多少行数据。显示的数据是一个估计值
  "avgObjSize" : 100.36363636363636,表示每行数据是大小，也是估计值，单位是bytes
  "dataSize" : 2208, 示当前数据库所有数据的总大小，不是指占有磁盘大小。单位是bytes
  "storageSize" : 36864,  表示当前数据库占有磁盘大小，单位是bytes,因为mongodb有预分配空间机制，为了防止当有大量数据插入时对磁盘的压力,因此会事先多分配磁盘空间。
  "numExtents" : 5,
  "indexes" : 6,表示system.indexes表 索引个数
  "indexSize" : 49056,  表示索引占有磁盘大小。单位是bytes
  "fileSize" : 67108864, 表示当前数据库预分配的文件大小，例如test.0,test.1，不包括test.ns。
  "nsSizeMB" : 16,
  "extentFreeList" : {
          "num" : 0,
          "totalSize" : 0
  },
  "dataFileVersion" : {
          "major" : 4,
          "minor" : 22
  },
  "ok" : 1
}


db.people.stats({indexDetails:true});
还能看到更详细的信息，比如cache的情况等。

17. hash index
Hashed indexes use a hashing function to compute the hash of the value of the index field. 
The hashing function collapses embedded documents and computes the hash for the entire value 
but does not support multi-key (i.e. arrays) indexes. 
Specifically, creating a hashed index on a field that contains an array or attempting to 
insert an array into a hashed indexed field returns an error.

Hashed indexes support sharding using hashed shard keys. 
Hashed based sharding uses a hashed index of a field as the shard key to partition data across your sharded cluster.

create a hash index:
db.collection.createIndex( { _id: "hashed" } )

Starting with MongoDB 4.4, MongoDB supports creating compound indexes that include a single hashed field. 
To create a compound hashed index, specify hashed as the value of any single index key when creating the index:

db.collection.createIndex( { "fieldA" : 1, "fieldB" : "hashed", "fieldC" : -1 } )

