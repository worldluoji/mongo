1. questions
1)
Correct answer:

Replica sets provide high availability.

Replica sets use failover to provide high availability to client applications.

Incorrect answers:

We can have up to 50 voting members in a replica set.

We can have up to 50 replica set members, but only 7 of those will be voting members.

Replica set members have a fixed role assigned.

Replica sets ensure high availability. This implies that if a given node fails, regardless of its role, the availability of the system will be ensured by failing over the role of primary to an available secondary node through an election.

We should always use arbiters.

Arbiters should not be used lightly and their utilization in the set is highly discouraged.
不应轻易使用仲裁器，并且强烈建议不要在集合中使用它们.

2)
Correct answers:

Enabling internal authentication in a replica set implicitly enables client authentication.
This is true; if nodes authenticate to each other, clients must authenticate to the cluster.

When connecting to a replica set, the mongo shell will redirect the connection to the primary node.
This is true; even if an election is held and there is temporarily no primary, the shell will wait to connect until a primary is elected.

Incorrect answers:

All nodes in a replica set must be run on the same port.
This is incorrect; in fact, the nodes should be run on different ports. Ideally, they would each run on different machines.

rs.initiate() must be run on every node in the replica set.
This is incorrect; rs.initiate() should only be run on one node in the replica set.

2. How to config replicas

The configuration file for the first node (node1.conf):

storage:
  dbPath: /var/mongodb/db/node1
net:
  bindIp: 192.168.103.100,localhost
  port: 27011
security:
  authorization: enabled
  keyFile: /var/mongodb/pki/m103-keyfile
systemLog:
  destination: file
  path: /var/mongodb/db/node1/mongod.log
  logAppend: true
processManagement:
  fork: true
replication:
  replSetName: m103-example
Creating the keyfile and setting permissions on it:

 
sudo mkdir -p /var/mongodb/pki/
sudo chown vagrant:vagrant /var/mongodb/pki/
openssl rand -base64 741 > /var/mongodb/pki/m103-keyfile
chmod 400 /var/mongodb/pki/m103-keyfile
Creating the dbpath for node1:

 
mkdir -p /var/mongodb/db/node1
Starting a mongod with node1.conf:

 
mongod -f node1.conf
ing node1.conf to node2.conf and node3.conf:

 
cp node1.conf node2.conf
cp node2.conf node3.conf
Editing node2.conf using vi:

 
vi node2.conf
Saving the file and exiting vi:

 
:wq
node2.conf, after changing the dbpath, port, and logpath:

 
storage:
  dbPath: /var/mongodb/db/node2
net:
  bindIp: 192.168.103.100,localhost
  port: 27012
security:
  keyFile: /var/mongodb/pki/m103-keyfile
systemLog:
  destination: file
  path: /var/mongodb/db/node2/mongod.log
  logAppend: true
processManagement:
  fork: true
replication:
  replSetName: m103-example
node3.conf, after changing the dbpath, port, and logpath:

 
storage:
  dbPath: /var/mongodb/db/node3
net:
  bindIp: 192.168.103.100,localhost
  port: 27013
security:
  keyFile: /var/mongodb/pki/m103-keyfile
systemLog:
  destination: file
  path: /var/mongodb/db/node3/mongod.log
  logAppend: true
processManagement:
  fork: true
replication:
  replSetName: m103-example
Creating the data directories for node2 and node3:

 
mkdir /var/mongodb/db/{node2,node3}
Starting mongod processes with node2.conf and node3.conf:

 
mongod -f node2.conf
mongod -f node3.conf
Connecting to node1:

 
mongo --port 27011
Initiating the replica set:

 
rs.initiate()
Creating a user:

 
use admin
db.createUser({
  user: "m103-admin",
  pwd: "m103-pass",
  roles: [
    {role: "root", db: "admin"}
  ]
})
Exiting out of the Mongo shell and connecting to the entire replica set:

 
exit
mongo --host "m103-example/192.168.103.100:27011" -u "m103-admin"
-p "m103-pass" --authenticationDatabase "admin"
Getting replica set status:

 
rs.status()
Adding other members to replica set:

 
rs.add("m103:27012")
rs.add("m103:27013")
这里“m103”在实际操作时要换为mongodb的具体IP或域名

Getting an overview of the replica set topology:

 
rs.isMaster()
Stepping down the current primary:

 
rs.stepDown()
Checking replica set overview after election:


rs.isMaster()

3. Replication configuration document
Replication configuration document is used to configure our replica sets.
This is where the properties of our replica sets are defined,
and the document is shared across all members of the set.

You can access the configuration of a replica set using the rs.conf() method or 
the replSetGetConfig command.

To modify the configuration for a replica set, use the rs.reconfig() method, 
passing a configuration document to the method. 


The members field is where a bunch of our basic configuration is going to be determined-- 
which nodes are part of the set, what roles do they have, 
and what kind of topology we want to define is set on this field.

_id field is the name of the replica set.

The next field is version.
Now, a version is just an integer that gets incremented every time 
the current configuration of our replica set changes.

There is a vast amount of other configuration options that either deal with
internal replication mechanisms or overall configurations of the sets.

https://docs.mongodb.com/manual/reference/replica-configuration/


4. replication commands
rs.status()
rs.status is used to report on the general health of each node in the set.

rs.isMaster()
This one describes the role of the node where we ran this command from.
And it also gives us some information about the replica set itself.

Atlas atlas-zdff09-shard-0 [primary] myFirstDatabase> rs.isMaster();
{
  topologyVersion: {
    processId: ObjectId("619402010462947bba914973"),
    counter: Long("6")
  },
  hosts: [
    'luoji-mongo-cluster-shard-00-00.ejuuu.mongodb.net:27017',
    'luoji-mongo-cluster-shard-00-01.ejuuu.mongodb.net:27017',
    'luoji-mongo-cluster-shard-00-02.ejuuu.mongodb.net:27017'
  ],
  setName: 'atlas-zdff09-shard-0',
  setVersion: 6,
  ismaster: true,
  secondary: false,
  primary: 'luoji-mongo-cluster-shard-00-01.ejuuu.mongodb.net:27017',
  tags: {
    provider: 'AWS',
    nodeType: 'ELECTABLE',
    region: 'AP_SOUTHEAST_1',
    workloadType: 'OPERATIONAL'
  },
  ......
 
db.serverStatus()['repl']
This command gives us a lot of information about the Mongo D process, but we're just going to look at the section called repl.
The output from this command is going to be very similar to the output of rs.isMaster.
 
rs.printReplicationInfo()
This command only has data about the oplog and specifically only the oplog for the node we're currently connected to.
It'll give us exact time stamps for the first and last events that occurred in the oplog for that node.

即只会打印出第一次和最后一次操作的时间
rs.printReplicationInfo() will only return timestamps for oplog statements; 
the statements themselves can be found in local.oplog.rs.
rs.printReplicationInfo() only contains information pertaining to the node where the command was run.


5. local db
Every mongod instance has its own local database, 
which stores data used in the replication process, and other instance-specific data. 
The local database is invisible to replication: collections in the local database are not replicated.

Atlas atlas-zdff09-shard-0 [primary] myFirstDatabase> show dbs;
m201                 101 MB
sample_airbnb       12.3 kB
sample_analytics    9.52 MB
sample_geospatial   1.06 MB
sample_mflix        50.2 MB
sample_restaurants  6.24 MB
sample_supplies     1.17 MB
sample_training     47.4 MB
sample_weatherdata   2.7 MB
admin                336 kB
local                821 MB

里面的local就是 local database.

Atlas atlas-zdff09-shard-0 [primary] local> show collections;
clustermanager
oplog.rs
replset.election
replset.initialSyncId
replset.minvalid
replset.oplogTruncateAfterPoint
startup_log

https://docs.mongodb.com/manual/reference/local-database/


Correct answers:

The local database will not be replicated.

Any data written to this database will not be replicated across the different nodes of the set.

The oplog.rs collection contains all operations that will be replicated.

The oplog.rs collection holds all the statements that get replicated across the different replica set members.

Incorrect answers:

You cannot write to the local database.

Given the correct permissions, an authorized user can write data to the local db. That said, we strongly advise against that.

The local database does not allow the creation of other collections.

Although we discourage it, you can write new collections to the local database.

We should drop the oplog.rs collection from time to time to avoid it becoming too big.

We cap the oplog.rs collection instead of dropping it entirely.


6. reconfig
It does not require any of the nodes to restarted.
When we reconfigure a replica set with rs.reconfig(), we do not need to restart any of the individual nodes.

It does not require any of the configuration files to be updated.
When we reconfigure a replica set with rs.reconfig(), we do not need to update any of the nodes' configuration files.

When issuing an updated configuration to rs.reconfig(), the entire configuration document is required.

Storing replica set configuration as a variable cfg:
cfg = rs.conf()

Setting the priority of a node to 0, so it cannot become primary (making the node "passive"): 
cfg.members[2].priority = 0


Updating our replica set to use the new configuration cfg:
rs.reconfig(cfg)


Checking the new topology of our set:
rs.isMaster()

Forcing an election in this replica set (although in this case, we rigged the election so only one node could become primary):
rs.stepDown()

Checking the topology of our set after the election:
rs.isMaster()


7. failover and election
Correct answers:

Nodes with priority 0 cannot be elected primary.

Setting a node's priority to 0 guarantees that node will never become primary.

Nodes with higher priority are more likely to be elected primary.

Raising a node's priority doesn't guarantee that node will be elected primary, but it does increase the likelihood.

Incorrect answers:

Elections can take place anytime while the primary is available.

If a majority of nodes are unavailable, elections cannot take place.

All nodes have an equal chance to become primary.

Priority and recency of a node's oplog dictates which nodes are more likely to become primary.


To have a primary you need a majority number of nodes up. 
If 2 nodes are down with a 3 nodes replica set, then you cannot have majority so you have no primary.

In MongoDB, the primary is elected by conducting an election among the competing nodes. A node must get majority of the votes.

A replica set with 3 voting members, Primary-Secondary-Secondary (P-S-S):

The majority of all voting members is 2.
The number of all data-bearing voting members is 3.
So, a node must get 2 votes to become the Primary. However, when 2 nodes are down, the remaining node can never become Primary.

If the remaining node was in the Primary state, it will step down and it will transition to a secondary state.

If the remaining node was in the Secondary state - nothing will happen.

So, we can say there will be no Primary when 2 nodes are down in the given setup.

Now, I would like you to think more about it and try to answer the lab questions based on the information that I have shared with you.


8. Problem

Evaluate the effect of using a write concern with a replica set where one node has failed.

Consider a 3-node replica set with only 2 healthy nodes, that receives the following insert() operation:

use payroll
db.employees.insert(
  { "name": "Aditya", "salary_USD": 50000 },
  { "writeConcern": { "w": 3, "wtimeout": 1000 } }
)
Which of the following is true about this write operation?

1) When a writeConcernError occurs, the document is still written to the healthy nodes.
This is correct.
The WriteResult object simply tells us whether the writeConcern was successful or not - it will not undo successful writes from any of the nodes.

2) The unhealthy node will have the inserted document when it is brought back online.
This is correct.
When the unhealthy node comes back online, it rejoins the replica set and its oplog is compared with the other nodes' oplogs. 
Any missing operations will be replayed on the newly healthy node.

3) w: "majority" would also cause this write operation to return with an error.
w: "majority" requests acknowledgement that a majority of nodes in a replica set have registered the write. 
In a three-node replica set, only two nodes are required for a majority, so the two healthy nodes are sufficient to satisfy this writeConcern.
The write operation will always return with an error, even if wtimeout is not specified.

4) If wtimeout is not specified, the write operation will be retried for an indefinite amount of time until the writeConcern is successful. 
If the writeConcern is impossible, like in this example, it may never return anything to the client.